{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a968b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129597aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv(\"data/Train.csv\")\n",
    "Test = pd.read_csv(\"data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d780347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical features ['user_id', 'REGION', 'TENURE', 'MRG', 'TOP_PACK']\n",
      "\n",
      "numerical features ['MONTANT', 'FREQUENCE_RECH', 'REVENUE', 'ARPU_SEGMENT', 'FREQUENCE', 'DATA_VOLUME', 'ON_NET', 'ORANGE', 'TIGO', 'ZONE1', 'ZONE2', 'REGULARITY', 'FREQ_TOP_PACK', 'CHURN']\n"
     ]
    }
   ],
   "source": [
    "#what features are categorical?\n",
    "categorical_features = Train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "#what features are numerical?\n",
    "numerical_features = Train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "print(\"categorical features\", categorical_features)\n",
    "print()\n",
    "print(\"numerical features\",numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa43680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "defective_features = ['user_id', 'MRG',\"TOP_PACK\"]\n",
    "Train.drop(defective_features, 1, inplace = True)\n",
    "Test.drop(defective_features, 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55038429",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = Train.shape[0]\n",
    "ntest = Test.shape[0]\n",
    "data = pd.concat((Train, Test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e74bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AVERAGE OF THE TENURE BOUNDARIES\n",
    "data['TENURE_avg'] = data['TENURE'].map({'K > 24 month': (24+27)/2, 'I 18-21 month':(18+21)/2 , 'H 15-18 month': (15+18)/2, 'G 12-15 month':(12+15)/2,\n",
    "                                             'J 21-24 month': (21+24)/2, 'F 9-12': (9+12)/2, 'E 6-9 month':(6+9)/2, 'D 3-6 month':(3+6)/2})\n",
    "\n",
    "data['TENURE'] = data['TENURE'].map({'K > 24 month': 24, 'I 18-21 month': 18, 'H 15-18 month': 15, 'G 12-15 month':12,\n",
    "                                             'J 21-24 month': 21, 'F 9-12': 9, 'E 6-9 month':6, 'D 3-6 month':3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53c27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "data['REGION_encoded']=le.fit_transform(data['REGION'])\n",
    "data.drop(['REGION'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82c13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Total_income'] = data['REVENUE'] * data['FREQUENCE']\n",
    "data['FREQ_PACK'] = data.FREQUENCE_RECH/data.FREQ_TOP_PACK\n",
    "data['diff//freq'] = (data['MONTANT'] - data['FREQUENCE_RECH']) / data['FREQUENCE']\n",
    "data['NOT_FREQUENCE_RECH'] = data['FREQUENCE_RECH'] - data['FREQ_TOP_PACK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e4f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diff_Orange'] = np.abs(data['ON_NET']-data['ORANGE'])\n",
    "data['diff_Tigo'] = np.abs(data['ON_NET']-data['TIGO'])\n",
    "data['freq//rech'] = data['FREQUENCE'] / data['FREQUENCE_RECH']\n",
    "data['freq//montant'] =  data['MONTANT']/ data['FREQUENCE']\n",
    "data['freq//revenue'] = data['FREQUENCE'] / data['REVENUE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66aa1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['segment/reg'] = data['ARPU_SEGMENT'] / data['REGULARITY']\n",
    "data['net//reg'] = data['ON_NET'] / data['REGULARITY']\n",
    "data['data//reg'] = data['DATA_VOLUME'] / data['REGULARITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bfe2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:ntrain]\n",
    "test_data = data[ntrain:]\n",
    "target = train_data['CHURN']\n",
    "train_data.drop([\"CHURN\"],1,inplace=True)\n",
    "test_data.drop([\"CHURN\"],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672a56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66debcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregate data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, target, test_size=0.33, stratify=target,random_state=56)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b3dfffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcba44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(estimator,data):\n",
    "    predictions = estimator.predict_proba(data)[:,1]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4285ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize models and their hyper-parameters\n",
    "xgb_model = xgb.XGBClassifier(n_jobs=-1,random_state=23,objective='binary:logistic',\n",
    "                n_estimators=2500,learning_rate=0.01,\n",
    "                colsample_bytree=0.9,subsample=1, use_label_encoder=False)\n",
    "cat_model = CatBoostClassifier(random_seed=34,use_best_model=True,\n",
    "                          n_estimators=5000,silent=True,eval_metric='Logloss')\n",
    "lgb = lgbm.LGBMClassifier(random_state=734, n_estimators=5000,num_leaves=120,learning_rate=0.008,\n",
    "    max_depth=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50015701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Fold:1====================\n",
      "[0]\tvalidation_0-logloss:0.68646\n",
      "[250]\tvalidation_0-logloss:0.27266\n",
      "[500]\tvalidation_0-logloss:0.25382\n",
      "[750]\tvalidation_0-logloss:0.25295\n",
      "[964]\tvalidation_0-logloss:0.25300\n",
      "Log loss for 1: 0.25294188923595684\n",
      "================Fold:2====================\n",
      "[0]\tvalidation_0-logloss:0.68648\n",
      "[250]\tvalidation_0-logloss:0.27354\n",
      "[500]\tvalidation_0-logloss:0.25427\n",
      "[750]\tvalidation_0-logloss:0.25324\n",
      "[953]\tvalidation_0-logloss:0.25325\n",
      "Log loss for 2: 0.253237936310374\n",
      "================Fold:3====================\n",
      "[0]\tvalidation_0-logloss:0.68649\n",
      "[250]\tvalidation_0-logloss:0.27385\n",
      "[500]\tvalidation_0-logloss:0.25463\n",
      "[750]\tvalidation_0-logloss:0.25363\n",
      "[1000]\tvalidation_0-logloss:0.25364\n",
      "[1011]\tvalidation_0-logloss:0.25365\n",
      "Log loss for 3: 0.25360825973937334\n",
      "================Fold:4====================\n",
      "[0]\tvalidation_0-logloss:0.68648\n",
      "[250]\tvalidation_0-logloss:0.27200\n",
      "[500]\tvalidation_0-logloss:0.25231\n",
      "[750]\tvalidation_0-logloss:0.25119\n",
      "[944]\tvalidation_0-logloss:0.25127\n",
      "Log loss for 4: 0.251189002210279\n",
      "================Fold:5====================\n",
      "[0]\tvalidation_0-logloss:0.68647\n",
      "[250]\tvalidation_0-logloss:0.27262\n",
      "[500]\tvalidation_0-logloss:0.25341\n",
      "[750]\tvalidation_0-logloss:0.25244\n",
      "[1000]\tvalidation_0-logloss:0.25243\n",
      "[1081]\tvalidation_0-logloss:0.25243\n",
      "Log loss for 5: 0.2524197265919445\n",
      "The Mean Log_loss eror: 0.2526793628175855\n"
     ]
    }
   ],
   "source": [
    "#Define training loop\n",
    "def train(estimator):\n",
    "    fold=0\n",
    "    scores,pp=[],[]\n",
    "    for train_index, test_index in skf.split(X_train,y_train):\n",
    "        fold+=1\n",
    "        print(f\"================Fold:{fold}====================\")\n",
    "        xtrain, xtest = X_train.iloc[train_index],X_train.iloc[test_index]\n",
    "        ytrain, ytest = y_train.iloc[train_index],y_train.iloc[test_index]\n",
    "        model = estimator.fit(xtrain,ytrain,early_stopping_rounds=200,eval_metric=\"logloss\",\n",
    "                      eval_set=[(xtest,ytest)],verbose=250)\n",
    "        prediction = model.predict_proba(xtest)\n",
    "        predict_ = model.predict_proba(test_data)\n",
    "        score = log_loss(ytest,prediction)\n",
    "        print(f\"Log loss for {fold}: {score}\")\n",
    "        scores.append(score)\n",
    "        pp.append(predict_)\n",
    "\n",
    "        #0.2528473\n",
    "      #Baseline Mean: 0.25479403  \n",
    "    print(f\"The Mean Log_loss eror: {np.mean(scores)}\")##0.252703\n",
    "    return pp\n",
    "\n",
    "#Train Xgboost model\n",
    "xgb_pred = train(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d2257f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Fold:1====================\n",
      "[250]\tvalid_0's binary_logloss: 0.265241\n",
      "[500]\tvalid_0's binary_logloss: 0.253859\n",
      "[750]\tvalid_0's binary_logloss: 0.253377\n",
      "Log loss for 1: 0.2533572813268959\n",
      "================Fold:2====================\n",
      "[250]\tvalid_0's binary_logloss: 0.266122\n",
      "[500]\tvalid_0's binary_logloss: 0.25471\n",
      "[750]\tvalid_0's binary_logloss: 0.253966\n",
      "Log loss for 2: 0.25394878737305215\n",
      "================Fold:3====================\n",
      "[250]\tvalid_0's binary_logloss: 0.266414\n",
      "[500]\tvalid_0's binary_logloss: 0.254766\n",
      "[750]\tvalid_0's binary_logloss: 0.254038\n",
      "Log loss for 3: 0.2540358492588115\n",
      "================Fold:4====================\n",
      "[250]\tvalid_0's binary_logloss: 0.264497\n",
      "[500]\tvalid_0's binary_logloss: 0.252514\n",
      "[750]\tvalid_0's binary_logloss: 0.251721\n",
      "Log loss for 4: 0.25170799728443594\n",
      "================Fold:5====================\n",
      "[250]\tvalid_0's binary_logloss: 0.265108\n",
      "[500]\tvalid_0's binary_logloss: 0.253596\n",
      "[750]\tvalid_0's binary_logloss: 0.252906\n",
      "[1000]\tvalid_0's binary_logloss: 0.252844\n",
      "Log loss for 5: 0.25280949433577143\n",
      "The Mean Log_loss eror: 0.25317188191579343\n"
     ]
    }
   ],
   "source": [
    "#Train LightGBM model\n",
    "lgb_pred = train(lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba7dd4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Fold:1====================\n",
      "Learning rate set to 0.059247\n",
      "0:\tlearn: 0.6059764\ttest: 0.6057445\tbest: 0.6057445 (0)\ttotal: 207ms\tremaining: 17m 13s\n",
      "250:\tlearn: 0.2490748\ttest: 0.2524931\tbest: 0.2524891 (247)\ttotal: 18s\tremaining: 5m 39s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.2524746059\n",
      "bestIteration = 290\n",
      "\n",
      "Shrink model to first 291 iterations.\n",
      "Log loss for 1: 0.2524746058994139\n",
      "================Fold:2====================\n",
      "Learning rate set to 0.059247\n",
      "0:\tlearn: 0.6061231\ttest: 0.6064137\tbest: 0.6064137 (0)\ttotal: 63.9ms\tremaining: 5m 19s\n",
      "250:\tlearn: 0.2490118\ttest: 0.2530929\tbest: 0.2530709 (226)\ttotal: 17.1s\tremaining: 5m 23s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.2530708805\n",
      "bestIteration = 226\n",
      "\n",
      "Shrink model to first 227 iterations.\n",
      "Log loss for 2: 0.25307088046367776\n",
      "================Fold:3====================\n",
      "Learning rate set to 0.059247\n",
      "0:\tlearn: 0.6057300\ttest: 0.6058723\tbest: 0.6058723 (0)\ttotal: 62.1ms\tremaining: 5m 10s\n",
      "250:\tlearn: 0.2490444\ttest: 0.2530026\tbest: 0.2529925 (244)\ttotal: 17.3s\tremaining: 5m 27s\n",
      "500:\tlearn: 0.2458821\ttest: 0.2529404\tbest: 0.2528520 (384)\ttotal: 34.1s\tremaining: 5m 5s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.2528519694\n",
      "bestIteration = 384\n",
      "\n",
      "Shrink model to first 385 iterations.\n",
      "Log loss for 3: 0.2528519693524294\n",
      "================Fold:4====================\n",
      "Learning rate set to 0.059247\n",
      "0:\tlearn: 0.6057704\ttest: 0.6058576\tbest: 0.6058576 (0)\ttotal: 65.2ms\tremaining: 5m 25s\n",
      "250:\tlearn: 0.2495100\ttest: 0.2510382\tbest: 0.2510358 (247)\ttotal: 17.2s\tremaining: 5m 26s\n",
      "500:\tlearn: 0.2460842\ttest: 0.2511151\tbest: 0.2510216 (383)\ttotal: 34s\tremaining: 5m 5s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.2510215718\n",
      "bestIteration = 383\n",
      "\n",
      "Shrink model to first 384 iterations.\n",
      "Log loss for 4: 0.2510215717520977\n",
      "================Fold:5====================\n",
      "Learning rate set to 0.059247\n",
      "0:\tlearn: 0.6058849\ttest: 0.6059520\tbest: 0.6059520 (0)\ttotal: 63.3ms\tremaining: 5m 16s\n",
      "250:\tlearn: 0.2490817\ttest: 0.2525780\tbest: 0.2525739 (232)\ttotal: 17.5s\tremaining: 5m 31s\n",
      "500:\tlearn: 0.2455256\ttest: 0.2525469\tbest: 0.2525132 (326)\ttotal: 34.3s\tremaining: 5m 7s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.252513188\n",
      "bestIteration = 326\n",
      "\n",
      "Shrink model to first 327 iterations.\n",
      "Log loss for 5: 0.25251318801630546\n",
      "The Mean Log_loss eror: 0.2523864430967848\n"
     ]
    }
   ],
   "source": [
    "#Define training loop\n",
    "fold=0\n",
    "scores,pp=[],[]\n",
    "estimator=cat_model\n",
    "for train_index, test_index in skf.split(X_train,y_train):\n",
    "    fold+=1\n",
    "    print(f\"================Fold:{fold}====================\")\n",
    "    xtrain, xtest = X_train.iloc[train_index],X_train.iloc[test_index]\n",
    "    ytrain, ytest = y_train.iloc[train_index],y_train.iloc[test_index]\n",
    "    model = estimator.fit(xtrain,ytrain,early_stopping_rounds=200,\n",
    "                  eval_set=[(xtest,ytest)],verbose=250)\n",
    "    prediction = model.predict_proba(xtest)\n",
    "    predict_ = model.predict_proba(test_data)\n",
    "    score = log_loss(ytest,prediction)\n",
    "    print(f\"Log loss for {fold}: {score}\")\n",
    "    scores.append(score)\n",
    "    pp.append(predict_)\n",
    "\n",
    "  \n",
    "print(f\"The Mean Log_loss eror: {np.mean(scores)}\")##0.252703\n",
    "cat_predict=pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5909edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate predictions for all three models\n",
    "cat_pred = np.mean(cat_predict,0)[:,1]\n",
    "lgb_pred = np.mean(lgb_pred,0)[:,1]\n",
    "xgb_pred = np.mean(xgb_pred,0)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2ef96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better perfromance a blend of all three model's prediction is required.\n",
    "#Weights (e.g 0.6) are assigned on the basis of individual perfromance.\n",
    "predictions=(((0.6*cat_pred) + (0.3*lgb_pred) + (0.1*cat_pred)) + ((0.45*cat_pred) + (0.55*lgb_pred)))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2151ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ubmit predictions\n",
    "ss= pd.read_csv(\"DSN_PreBootcamp_Hackathon/sample_submission.csv\")\n",
    "ss['CHURN'] = predictions\n",
    "ss.to_csv(\"Submission_lgb.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
