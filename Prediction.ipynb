{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a968b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "129597aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv(\"data/Train.csv\")\n",
    "Test = pd.read_csv(\"data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d780347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical features ['user_id', 'REGION', 'TENURE', 'MRG', 'TOP_PACK']\n",
      "\n",
      "numerical features ['MONTANT', 'FREQUENCE_RECH', 'REVENUE', 'ARPU_SEGMENT', 'FREQUENCE', 'DATA_VOLUME', 'ON_NET', 'ORANGE', 'TIGO', 'ZONE1', 'ZONE2', 'REGULARITY', 'FREQ_TOP_PACK', 'CHURN']\n"
     ]
    }
   ],
   "source": [
    "#what features are categorical?\n",
    "categorical_features = Train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "#what features are numerical?\n",
    "numerical_features = Train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "print(\"categorical features\", categorical_features)\n",
    "print()\n",
    "print(\"numerical features\",numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa43680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "defective_features = ['user_id', 'MRG',\"TOP_PACK\"]\n",
    "Train.drop(defective_features, 1, inplace = True)\n",
    "Test.drop(defective_features, 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55038429",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = Train.shape[0]\n",
    "ntest = Test.shape[0]\n",
    "data = pd.concat((Train, Test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e74bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AVERAGE OF THE TENURE BOUNDARIES\n",
    "data['TENURE_avg'] = data['TENURE'].map({'K > 24 month': (24+27)/2, 'I 18-21 month':(18+21)/2 , 'H 15-18 month': (15+18)/2, 'G 12-15 month':(12+15)/2,\n",
    "                                             'J 21-24 month': (21+24)/2, 'F 9-12': (9+12)/2, 'E 6-9 month':(6+9)/2, 'D 3-6 month':(3+6)/2})\n",
    "\n",
    "data['TENURE'] = data['TENURE'].map({'K > 24 month': 24, 'I 18-21 month': 18, 'H 15-18 month': 15, 'G 12-15 month':12,\n",
    "                                             'J 21-24 month': 21, 'F 9-12': 9, 'E 6-9 month':6, 'D 3-6 month':3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d53c27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "data['REGION_encoded']=le.fit_transform(data['REGION'])\n",
    "data.drop(['REGION'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d82c13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Total_income'] = data['REVENUE'] * data['FREQUENCE']\n",
    "data['FREQ_PACK'] = data.FREQUENCE_RECH/data.FREQ_TOP_PACK\n",
    "data['diff//freq'] = (data['MONTANT'] - data['FREQUENCE_RECH']) / data['FREQUENCE']\n",
    "data['NOT_FREQUENCE_RECH'] = data['FREQUENCE_RECH'] - data['FREQ_TOP_PACK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8e4f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diff_Orange'] = np.abs(data['ON_NET']-data['ORANGE'])\n",
    "data['diff_Tigo'] = np.abs(data['ON_NET']-data['TIGO'])\n",
    "data['freq//rech'] = data['FREQUENCE'] / data['FREQUENCE_RECH']\n",
    "data['freq//montant'] =  data['MONTANT']/ data['FREQUENCE']\n",
    "data['freq//revenue'] = data['FREQUENCE'] / data['REVENUE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66aa1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['segment/reg'] = data['ARPU_SEGMENT'] / data['REGULARITY']\n",
    "data['net//reg'] = data['ON_NET'] / data['REGULARITY']\n",
    "data['data//reg'] = data['DATA_VOLUME'] / data['REGULARITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bfe2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:ntrain]\n",
    "test_data = data[ntrain:]\n",
    "target = train_data['CHURN']\n",
    "train_data.drop([\"CHURN\"],1,inplace=True)\n",
    "test_data.drop([\"CHURN\"],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "672a56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66debcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, target, test_size=0.33, stratify=target,random_state=56)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b3dfffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50015701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Fold:1====================\n",
      "[18:36:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68646\n",
      "[250]\tvalidation_0-logloss:0.27266\n",
      "[500]\tvalidation_0-logloss:0.25382\n",
      "[750]\tvalidation_0-logloss:0.25292\n",
      "[976]\tvalidation_0-logloss:0.25296\n",
      "Log loss for 1: 0.25290475368811427\n",
      "================Fold:2====================\n",
      "[18:43:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68648\n",
      "[250]\tvalidation_0-logloss:0.27353\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ALEXAN~1\\AppData\\Local\\Temp/ipykernel_13204/3950247102.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 colsample_bytree=0.9,subsample=1, use_label_encoder=False)\n\u001b[1;32m---> 11\u001b[1;33m     xgb_model.fit(xtrain,ytrain,early_stopping_rounds=200,\n\u001b[0m\u001b[0;32m     12\u001b[0m                   eval_set=[(xtest,ytest)],verbose=250)\n\u001b[0;32m     13\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1248\u001b[0m         )\n\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m         self._Booster = train(\n\u001b[0m\u001b[0;32m   1251\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \"\"\"\n\u001b[1;32m--> 188\u001b[1;33m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    189\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1680\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1681\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fold=0\n",
    "scores=[]\n",
    "for train_index, test_index in skf.split(X_train,y_train):\n",
    "    fold+=1\n",
    "    print(f\"================Fold:{fold}====================\")\n",
    "    xtrain, xtest = X_train.iloc[train_index],X_train.iloc[test_index]\n",
    "    ytrain, ytest = y_train.iloc[train_index],y_train.iloc[test_index]\n",
    "    xgb_model = xgb.XGBClassifier(n_jobs=-1,random_state=23,objective='binary:logistic',\n",
    "                n_estimators=2500,learning_rate=0.01,\n",
    "                colsample_bytree=0.9,subsample=1, use_label_encoder=False)\n",
    "    xgb_model.fit(xtrain,ytrain,early_stopping_rounds=200,\n",
    "                  eval_set=[(xtest,ytest)],verbose=250)\n",
    "    prediction = xgb_model.predict_proba(xtest)\n",
    "    score = log_loss(ytest,prediction)\n",
    "    print(f\"Log loss for {fold}: {score}\")\n",
    "    scores.append(score)\n",
    "\n",
    "    \n",
    "  #Baseline Mean: 0.25479403  \n",
    "print(f\"The Mean Log_loss eror: {np.mean(scores)}\")##0.252703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d2257f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Fold:1====================\n",
      "Learning rate set to 0.059247\n",
      "0:\tlearn: 0.6051587\ttest: 0.6049709\tbest: 0.6049709 (0)\ttotal: 69.4ms\tremaining: 5m 47s\n",
      "250:\tlearn: 0.2490721\ttest: 0.2525302\tbest: 0.2525290 (243)\ttotal: 22.5s\tremaining: 7m 5s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.2525037667\n",
      "bestIteration = 283\n",
      "\n",
      "Shrink model to first 284 iterations.\n",
      "Log loss for 1: 0.2525037667456006\n",
      "================Fold:2====================\n",
      "Learning rate set to 0.059247\n",
      "0:\tlearn: 0.6054821\ttest: 0.6058315\tbest: 0.6058315 (0)\ttotal: 195ms\tremaining: 16m 15s\n",
      "250:\tlearn: 0.2488988\ttest: 0.2530742\tbest: 0.2530736 (246)\ttotal: 22.7s\tremaining: 7m 10s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.2530611516\n",
      "bestIteration = 258\n",
      "\n",
      "Shrink model to first 259 iterations.\n",
      "Log loss for 2: 0.25306115155013154\n",
      "================Fold:3====================\n",
      "Learning rate set to 0.059247\n",
      "0:\tlearn: 0.6051928\ttest: 0.6052670\tbest: 0.6052670 (0)\ttotal: 74.9ms\tremaining: 6m 14s\n",
      "250:\tlearn: 0.2490243\ttest: 0.2530962\tbest: 0.2530962 (250)\ttotal: 21.2s\tremaining: 6m 40s\n",
      "500:\tlearn: 0.2459040\ttest: 0.2529312\tbest: 0.2528757 (425)\ttotal: 42.4s\tremaining: 6m 20s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.252875675\n",
      "bestIteration = 425\n",
      "\n",
      "Shrink model to first 426 iterations.\n",
      "Log loss for 3: 0.2528756750084833\n",
      "================Fold:4====================\n",
      "Learning rate set to 0.059247\n",
      "0:\tlearn: 0.6049941\ttest: 0.6051155\tbest: 0.6051155 (0)\ttotal: 77.6ms\tremaining: 6m 27s\n",
      "250:\tlearn: 0.2494790\ttest: 0.2508043\tbest: 0.2508014 (247)\ttotal: 23.7s\tremaining: 7m 28s\n",
      "500:\tlearn: 0.2460082\ttest: 0.2509168\tbest: 0.2507676 (369)\ttotal: 45.7s\tremaining: 6m 50s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.2507675528\n",
      "bestIteration = 369\n",
      "\n",
      "Shrink model to first 370 iterations.\n",
      "Log loss for 4: 0.25076755277997764\n",
      "================Fold:5====================\n",
      "Learning rate set to 0.059247\n",
      "0:\tlearn: 0.6052538\ttest: 0.6052775\tbest: 0.6052775 (0)\ttotal: 128ms\tremaining: 10m 39s\n",
      "250:\tlearn: 0.2491165\ttest: 0.2526003\tbest: 0.2526003 (250)\ttotal: 32.2s\tremaining: 10m 8s\n",
      "500:\tlearn: 0.2456273\ttest: 0.2526200\tbest: 0.2525244 (301)\ttotal: 1m 6s\tremaining: 9m 53s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.2525243693\n",
      "bestIteration = 301\n",
      "\n",
      "Shrink model to first 302 iterations.\n",
      "Log loss for 5: 0.2525243693229884\n",
      "The Mean Log_loss eror: 0.2523465030814363\n"
     ]
    }
   ],
   "source": [
    "fold=0\n",
    "scores,pp=[],[]\n",
    "for train_index, test_index in skf.split(X_train,y_train):\n",
    "    fold+=1\n",
    "    print(f\"================Fold:{fold}====================\")\n",
    "    xtrain, xtest = X_train.iloc[train_index],X_train.iloc[test_index]\n",
    "    ytrain, ytest = y_train.iloc[train_index],y_train.iloc[test_index]\n",
    "    cat_model = CatBoostClassifier(random_seed=34,use_best_model=True,\n",
    "                          n_estimators=5000,silent=True,eval_metric='Logloss')\n",
    "    cat_model.fit(xtrain,ytrain,eval_set=[(xtest,ytest)],early_stopping_rounds=200,\n",
    "                           verbose=250,use_best_model=True)\n",
    "    prediction = cat_model.predict_proba(xtest)\n",
    "    predict_ = cat_model.predict_proba(test_data)\n",
    "    score = log_loss(ytest,prediction)\n",
    "    print(f\"Log loss for {fold}: {score}\")\n",
    "    scores.append(score)\n",
    "    pp.append(predict_)\n",
    "\n",
    "    \n",
    "print(f\"The Mean Log_loss eror: {np.mean(scores)}\")#0.252346503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c1c96f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.062605\n",
      "0:\tlearn: 0.6006370\ttest: 0.6005943\tbest: 0.6005943 (0)\ttotal: 88.5ms\tremaining: 7m 22s\n",
      "250:\tlearn: 0.2494260\ttest: 0.2524416\tbest: 0.2524413 (249)\ttotal: 28.2s\tremaining: 8m 53s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.2524075021\n",
      "bestIteration = 281\n",
      "\n",
      "Shrink model to first 282 iterations.\n"
     ]
    }
   ],
   "source": [
    "cat_model = CatBoostClassifier(random_seed=34,use_best_model=True,\n",
    "                          n_estimators=5000,silent=True,eval_metric='Logloss')\n",
    "cat_model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=200,\n",
    "                       verbose=250,use_best_model=True)\n",
    "predictions_ = cat_model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2151ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss= pd.read_csv(\"DSN_PreBootcamp_Hackathon/sample_submission.csv\")\n",
    "ss['CHURN'] = np.mean(pp,0)[:,1]\n",
    "ss.to_csv(\"Submission_cat2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fcff4c3",
   "metadata": {},
   "source": [
    "xgb_model = xgb.XGBClassifier(n_jobs=-1,random_state=23,objective='binary:logistic',\n",
    "                n_estimators=3500,learning_rate=0.01,\n",
    "                colsample_bytree=0.9,subsample=1, use_label_encoder=False)\n",
    "xgb_model.fit(X_train,y_train,early_stopping_rounds=200,\n",
    "              eval_set=[(X_test,y_test)],verbose=250)\n",
    "preds = xgb_model.predict_proba(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f771e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss= pd.read_csv(\"DSN_PreBootcamp_Hackathon/sample_submission.csv\")\n",
    "ss['CHURN'] = preds[:,1]\n",
    "ss.to_csv(\"Submission_cat.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b6e584c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([325156,  74844], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073406ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
